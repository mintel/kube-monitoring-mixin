apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
  - name: node-exporter.rules
    rules:
    - expr: |
        count without (cpu) (
          count without (mode) (
            node_cpu_seconds_total{job="node-exporter"}
          )
        )
      record: instance:node_num_cpu:sum
    - expr: |
        1 - avg without (cpu, mode) (
          rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
        )
      record: instance:node_cpu_utilisation:rate1m
    - expr: |
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |
        1 - (
          node_memory_MemAvailable_bytes{job="node-exporter"}
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        )
      record: instance:node_memory_utilisation:ratio
    - expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
      record: instance:node_vmstat_pgmajfault:rate1m
    - expr: |
        rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"sda"}[1m])
      record: instance_device:node_disk_io_time_seconds:rate1m
    - expr: |
        rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"sda"}[1m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate1m
  - name: kube-prometheus-node-recording.rules
    rules:
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY
        (instance)
      record: instance:node_cpu:rate:sum
    - expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
        BY (instance)
      record: instance:node_filesystem_usage:sum
    - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
      record: instance:node_network_receive_bytes:rate:sum
    - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
      record: instance:node_network_transmit_bytes:rate:sum
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
        (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
        BY (instance, cpu)) BY (instance)
      record: instance:node_cpu:ratio
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
      record: cluster:node_cpu:sum_rate5m
    - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
        BY (instance, cpu))
      record: cluster:node_cpu:ratio
  - name: k8s.rules
    rules:
    - expr: |
        sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])) by (namespace)
      record: namespace:container_cpu_usage_seconds_total:sum_rate
    - expr: |
        sum by (cluster, namespace, pod, container) (
          rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])
        ) * on (cluster, namespace, pod) group_left(node) max by(cluster, namespace, pod, node) (kube_pod_info)
      record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
    - expr: |
        container_memory_working_set_bytes{job="kubelet", image!=""}
        * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
      record: node_namespace_pod_container:container_memory_working_set_bytes
    - expr: |
        container_memory_rss{job="kubelet", image!=""}
        * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
      record: node_namespace_pod_container:container_memory_rss
    - expr: |
        container_memory_cache{job="kubelet", image!=""}
        * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
      record: node_namespace_pod_container:container_memory_cache
    - expr: |
        container_memory_swap{job="kubelet", image!=""}
        * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
      record: node_namespace_pod_container:container_memory_swap
    - expr: |
        sum(container_memory_usage_bytes{job="kubelet", image!="", container!="POD"}) by (namespace)
      record: namespace:container_memory_usage_bytes:sum
    - expr: |
        sum by (namespace) (
            sum by (namespace, pod) (
                max by (namespace, pod, container) (
                    kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}
                ) * on(namespace, pod) group_left() max by (namespace, pod) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
    - expr: |
        sum by (namespace) (
            sum by (namespace, pod) (
                max by (namespace, pod, container) (
                    kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"}
                ) * on(namespace, pod) group_left() max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
    - expr: |
        sum(
          label_replace(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
              "replicaset", "$1", "owner_name", "(.*)"
            ) * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job="kube-state-metrics"},
            "workload", "$1", "owner_name", "(.*)"
          )
        ) by (cluster, namespace, workload, pod)
      labels:
        workload_type: deployment
      record: mixin_pod_workload
    - expr: |
        sum(
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        ) by (cluster, namespace, workload, pod)
      labels:
        workload_type: daemonset
      record: mixin_pod_workload
    - expr: |
        sum(
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        ) by (cluster, namespace, workload, pod)
      labels:
        workload_type: statefulset
      record: mixin_pod_workload
  - name: node.rules
    rules:
    - expr: |
        sum(min(kube_pod_info) by (cluster, node))
      record: ':kube_pod_info_node_count:'
    - expr: |
        max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |
        count by (cluster, node) (sum by (node, cpu) (
          node_cpu_seconds_total{job="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        ))
      record: node:node_num_cpu:sum
    - expr: |
        sum(
          node_memory_MemAvailable_bytes{job="node-exporter"} or
          (
            node_memory_Buffers_bytes{job="node-exporter"} +
            node_memory_Cached_bytes{job="node-exporter"} +
            node_memory_MemFree_bytes{job="node-exporter"} +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        ) by (cluster)
      record: :node_memory_MemAvailable_bytes:sum
  - name: blackbox.rules
    rules:
    - expr: count by (target,job,app_mintel_com_owner) (up{job="blackbox"})
      record: blackbox_node_count
  - name: elasticsearch.rules
    rules:
    - expr: 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes)
        / elasticsearch_filesystem_data_size_bytes
      record: elasticsearch_filesystem_data_used_percent
    - expr: 100 - elasticsearch_filesystem_data_used_percent
      record: elasticsearch_filesystem_data_free_percent
    - expr: count by (cluster,role,namespace,job) (elasticsearch_nodes_roles{role="client"}==1)
      record: elasticsearch_cluster_number_of_client_nodes
    - expr: count by (cluster,role,namespace,job) (elasticsearch_nodes_roles{role="ingest"}==1)
      record: elasticsearch_cluster_number_of_ingest_nodes
    - expr: count by (cluster,role,namespace,job) (elasticsearch_nodes_roles{role="master"}==1)
      record: elasticsearch_cluster_number_of_master_nodes
    - expr: count by (cluster,role,namespace,job) (elasticsearch_nodes_roles{role="data"}==1)
      record: elasticsearch_cluster_number_of_data_nodes
  - name: haproxy-ingress.rules
    rules:
    - expr: |
        label_replace(
        label_replace(http_backend_response_wait_seconds_bucket{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
        * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:http_backend_response_wait_seconds_bucket:labeled
    - expr: |
        label_replace(
        label_replace(http_backend_queue_time_seconds_bucket{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
          * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:http_backend_queue_time_seconds_bucket:labeled
    - expr: |
        label_replace(
        label_replace(haproxy_backend_current_queue{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
          * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:haproxy_backend_current_queue:labeled
    - expr: |
        label_replace(
        label_replace(haproxy_backend_response_errors_total{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
          * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:haproxy_backend_response_errors_total:labeled
    - expr: |
        label_replace(
        label_replace(haproxy_backend_http_responses_total{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
          * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:haproxy_backend_http_responses_total:labeled
    - expr: |
        label_replace(
        label_replace(haproxy_backend_up{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
          * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:haproxy_backend_up:labeled
    - expr: |
        label_replace(
        label_replace(haproxy_server_up{backend!~"(error|stats|.*default-backend)"}, "mintel_com_service", "$1", "backend", "(.*)-\\d+$")
          * on(mintel_com_service) group_left(label_app_mintel_com_owner)
          label_join(kube_service_labels, "mintel_com_service", "-", "namespace", "service"),
        "label_app_mintel_com_owner",
        "satoshi",
        "label_app_mintel_com_owner",
        ""
        )
      record: haproxy:haproxy_server_up:labeled
    - expr: |
        histogram_quantile(0.95, sum(rate(haproxy:http_backend_response_wait_seconds_bucket:labeled[5m])) by (mintel_com_service, le, label_app_mintel_com_owner))
      labels:
        quantile: "0.95"
      record: haproxy:http_backend_response_wait_seconds_bucket:histogram_quantile
    - expr: |
        histogram_quantile(0.95, sum(rate(haproxy:http_backend_queue_time_seconds_bucket:labeled[5m])) by (mintel_com_service, le, label_app_mintel_com_owner))
      labels:
        quantile: "0.95"
      record: haproxy:http_backend_queue_time_seconds_bucket:histogram_quantile
    - expr: "100 * (\n  sum by (frontend) (haproxy_frontend_current_sessions) \n  /
        \n  sum by (frontend) (haproxy_frontend_limit_sessions)\n)\n"
      record: haproxy:http_frontend_session_usage:percentage
  - name: mintel-disk.rules
    rules:
    - expr: |
        (kubelet_volume_stats_inodes_free{job="kubelet"} / kubelet_volume_stats_inodes{job="kubelet"}) * 100
      record: mintel:pvc:inodes_free:percentage
  - name: mintel-node.rules
    rules:
    - expr: 100 - ( (node_nf_conntrack_entries / node_nf_conntrack_entries_limit)
        * 100 )
      record: node:conntrack_entries_free:percentage
    - expr: |
        avg(irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"sda"}[1m]))
      record: :node_disk_utilisation:avg_irate
    - expr: "avg by (node) (\n  irate(node_disk_io_time_seconds_total{job=\"node-exporter\",device=~\"sda\"}[1m])\n*
        on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n)
        \n"
      record: node:node_disk_utilisation:avg_irate
    - expr: |
        avg(irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"sda"}[1m]))
      record: :node_disk_saturation:avg_irate
    - expr: |
        avg by (node) (
          irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"sda"}[1m])
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        )
      record: node:node_disk_saturation:avg_irate
    - expr: |
        max by (instance, namespace, pod, device) ((node_filesystem_size_bytes{fstype="ext4", mountpoint="/mnt/stateful_partition"}
        - node_filesystem_avail_bytes{fstype="ext4", mountpoint="/mnt/stateful_partition"})
        / node_filesystem_size_bytes{fstype="ext4", mountpoint="/mnt/stateful_partition"})
      record: 'node:node_filesystem_usage:'
    - expr: |
        max by (instance, namespace, pod, device) (node_filesystem_avail_bytes{fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_size_bytes{fstype="ext4", mountpoint="/mnt/stateful_partition"})
      record: 'node:node_filesystem_avail:'
    - expr: |
        max(
          max(
            kube_pod_info{job="kube-state-metrics", host_ip!=""}
          ) by (node, host_ip)
          * on (host_ip) group_right (node)
          label_replace(
            (max(node_filesystem_files{job="node-exporter", mountpoint="/mnt/stateful_partition"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
          )
        ) by (node)
      record: 'node:node_inodes_total:'
    - expr: |
        max(
          max(
            kube_pod_info{job="kube-state-metrics", host_ip!=""}
          ) by (node, host_ip)
          * on (host_ip) group_right (node)
          label_replace(
            (max(node_filesystem_files_free{job="node-exporter", mountpoint="/mnt/stateful_partition"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
          )
        ) by (node)
      record: 'node:node_inodes_free:'
  - name: mintel-overcommit.rules
    rules:
    - expr: |
        count by (zone) (label_replace(
          kube_node_info{job="kube-state-metrics"}, "zone", "$1", "provider_id","gce://.*/(.*)/.*")
        )
      record: mintel:cluster:nodes_per_zone:count
  - name: mintel-pod.rules
    rules:
    - expr: label_replace( (sum by (namespace, container_name, pod_name, environment)
        ( sum(container_memory_usage_bytes{container_name!=""}) by (container_name,
        pod_name) * on (pod_name) group_left(environment, namespace) label_replace(kube_pod_labels{label_app!=""},"pod_name","$1","pod","(.*)")
        ) / sum( label_replace( label_replace( kube_pod_container_resource_requests_memory_bytes{container!=""},
        "container_name", "$1", "container",  "(.*)" ), "pod_name", "$1", "pod", "(.*)"
        ) ) by (namespace, container_name, pod_name, environment) * 100), "service",
        "$1", "container_name", "(.*)")
      record: mintel:pod:usage_vs_request_memory:percent
    - expr: count by(created_by_kind, created_by_name, node) (kube_pod_info{created_by_kind!~"<none>|Job"})
      record: mintel:pod:allocation:node
    - expr: count by(created_by_kind, created_by_name, label_failure_domain_beta_kubernetes_io_zone)
        (kube_pod_info{created_by_kind!~"<none>|Job"} * on(node) group_left(label_failure_domain_beta_kubernetes_io_zone)
        kube_node_labels)
      record: mintel:pod:allocation:zone
    - expr: count by(created_by_kind, created_by_name) (kube_pod_info{created_by_kind!~"<none>|Job"})
      record: mintel:pod:allocation:totals
  - name: mintel-web-frontend.rules
    rules:
    - expr: "label_replace( \n  (up) * on (pod) \n    group_left(label_app_mintel_com_owner,label_app_mintel_com_pipeline_stage)
        \n      kube_pod_labels{label_app_mintel_com_owner=~\".+\", label_app_mintel_com_owner!~\"sre\",
        \ label_tier=\"frontend\", label_app_mintel_com_pipeline_stage=~\".+\"}, \n
        \ \"environment\",\n  \"$1\",\n  \"label_app_mintel_com_pipeline_stage\",\n
        \ \"(.*)\"\n)\n"
      record: mintel:web_frontend:check_up
  - name: node-exporter
    rules:
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left and is filling
          up.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_size_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 40
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left and is filling
          up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_size_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 20
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        page: "true"
        severity: critical
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
        summary: Filesystem has less than 5% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_size_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
        summary: Filesystem has less than 3% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_size_bytes{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        page: "true"
        severity: critical
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left and is filling
          up.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_files{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left and is filling
          up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_files{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        page: "true"
        severity: critical
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
        summary: Filesystem has less than 5% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_files{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
        summary: Filesystem has less than 3% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} / node_filesystem_files{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype="ext4", mountpoint="/mnt/stateful_partition"} == 0
        )
      for: 1h
      labels:
        page: "true"
        severity: critical
    - alert: NodeNetworkReceiveErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs
        summary: Network interface is reporting many receive errors.
      expr: |
        increase(node_network_receive_errs_total[2m]) > 10
      for: 1h
      labels:
        severity: warning
    - alert: NodeNetworkTransmitErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs
        summary: Network interface is reporting many transmit errors.
      expr: |
        increase(node_network_transmit_errs_total[2m]) > 10
      for: 1h
      labels:
        severity: warning
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerConfigInconsistent
      annotations:
        message: The configuration of the instances of the Alertmanager cluster `{{$labels.service}}`
          are out of sync.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerconfiginconsistent
      expr: |
        count_values("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="monitoring"}) BY (service) / ON(service) GROUP_LEFT() label_replace(max(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="monitoring",controller="alertmanager"}) by (name, job, namespace, controller), "service", "alertmanager-$1", "name", "(.*)") != 1
      for: 5m
      labels:
        page: "false"
        severity: critical
    - alert: AlertmanagerFailedReload
      annotations:
        message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
          }}/{{ $labels.pod}}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerfailedreload
      expr: |
        alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"} == 0
      for: 10m
      labels:
        severity: warning
    - alert: AlertmanagerMembersInconsistent
      annotations:
        message: Alertmanager has not found all other members of the cluster.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagermembersinconsistent
      expr: |
        alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}
          != on (service) GROUP_LEFT()
        count by (service) (alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"})
      for: 5m
      labels:
        page: "true"
        severity: critical
  - name: general.rules
    rules:
    - alert: TargetDown
      annotations:
        message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }} targets in
          {{ $labels.namespace }} namespace are down.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-targetdown
      expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,
        namespace, service)) > 10
      for: 10m
      labels:
        severity: warning
    - alert: Watchdog
      annotations:
        message: |
          This is an alert meant to ensure that the entire alerting pipeline is functional.
          This alert is always firing, therefore it should always be firing in Alertmanager
          and always fire against a receiver. There are integrations with various notification
          mechanisms that send a notification when this alert is not firing. For example the
          "DeadMansSnitch" integration in PagerDuty.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-watchdog
      expr: vector(1)
      labels:
        severity: none
  - name: node-network
    rules:
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        message: Network interface "{{ $labels.device }}" changing it's up status
          often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkinterfaceflapping
      expr: |
        changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
  - name: prometheus-operator
    rules:
    - alert: PrometheusOperatorReconcileErrors
      annotations:
        message: Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace
          }} Namespace.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatorreconcileerrors
      expr: |
        rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator",namespace="monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorNodeLookupErrors
      annotations:
        message: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatornodelookuperrors
      expr: |
        rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator",namespace="monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
          }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * 60 * 5 > 0
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubePodNotReady
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
          state for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: |
        sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
          }} does not match, this indicates that the Deployment has failed but has
          not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
      expr: |
        kube_deployment_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        message: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
          matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
      expr: |
        kube_deployment_spec_replicas{job="kube-state-metrics"}
          !=
        kube_deployment_status_replicas_available{job="kube-state-metrics"}
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
          not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
      expr: |
        kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
          !=
        kube_statefulset_status_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
          }} does not match, this indicates that the StatefulSet has failed but has
          not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
      expr: |
        kube_statefulset_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
          has not been rolled out.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
      expr: |
        max without (revision) (
          kube_statefulset_status_current_revision{job="kube-state-metrics"}
            unless
          kube_statefulset_status_update_revision{job="kube-state-metrics"}
        )
          *
        (
          kube_statefulset_replicas{job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
        )
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        message: Only {{ $value | humanizePercentage }} of the desired Pods of DaemonSet
          {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
      expr: |
        kube_daemonset_status_number_ready{job="kube-state-metrics"}
          /
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} < 1.00
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeContainerWaiting
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}}
          has been in waiting state for longer than 1 hour.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting
      expr: |
        sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics"}) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are not scheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
      expr: |
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are running where they are not supposed to run.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
      expr: |
        kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeCronJobRunning
      annotations:
        message: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
          than 1h to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
      expr: |
        time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
      for: 1h
      labels:
        severity: warning
    - alert: KubeJobCompletion
      annotations:
        message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
          than one hour to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
      expr: |
        kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
      expr: |
        kube_job_failed{job="kube-state-metrics"}  > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaReplicasMismatch
      annotations:
        message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the
          desired number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
      expr: |
        (kube_hpa_status_desired_replicas{job="kube-state-metrics"}
          !=
        kube_hpa_status_current_replicas{job="kube-state-metrics"})
          and
        changes(kube_hpa_status_current_replicas[15m]) == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaMaxedOut
      annotations:
        message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at
          max replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
      expr: |
        kube_hpa_status_current_replicas{job="kube-state-metrics"}
          ==
        kube_hpa_spec_max_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        message: Cluster has overcommitted CPU resource requests for Pods and cannot
          tolerate node failure.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
      expr: |
        sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          >
        (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemOvercommit
      annotations:
        message: Cluster has overcommitted memory resource requests for Pods and cannot
          tolerate node failure.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
      expr: |
        sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})
          /
        sum(kube_node_status_allocatable_memory_bytes)
          >
        (count(kube_node_status_allocatable_memory_bytes)-1)
          /
        count(kube_node_status_allocatable_memory_bytes)
      for: 5m
      labels:
        severity: warning
    - alert: KubeCPUOvercommit
      annotations:
        message: Cluster has overcommitted CPU resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemOvercommit
      annotations:
        message: Cluster has overcommitted memory resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
          /
        sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
          > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeQuotaExceeded
      annotations:
        message: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
      expr: |
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          > 0.90
      for: 15m
      labels:
        severity: warning
    - alert: CPUThrottlingHigh
      annotations:
        message: '{{ $value | humanizePercentage }} throttling of CPU in namespace
          {{ $labels.namespace }} for container {{ $labels.container }} in pod {{
          $labels.pod }}.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
      expr: |
        sum(increase(container_cpu_cfs_throttled_periods_total{container!="", container_name!~"ingress-default-backend|redis-exporter|metadata-proxy|autoscaler|metrics-server(-nanny)?"}[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{container_name!~"ingress-default-backend|redis-exporter|metadata-proxy|autoscaler|metrics-server(-nanny)?"}[5m])) by (container, pod, namespace)
          > ( 60 / 100 )
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-storage
    rules:
    - alert: KubePersistentVolumeUsageCritical
      annotations:
        message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
          }} free.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
      expr: |
        kubelet_volume_stats_available_bytes{job="kubelet"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet"}
          < 0.03
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubePersistentVolumeFullInFourDays
      annotations:
        message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is expected to fill up within four
          days. Currently {{ $value | humanizePercentage }} is available.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
      expr: |
        (
          kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
        ) < 0.15
        and
        predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
      for: 1h
      labels:
        page: "false"
        severity: critical
    - alert: KubePersistentVolumeErrors
      annotations:
        message: The persistent volume {{ $labels.persistentvolume }} has status {{
          $labels.phase }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
      expr: |
        kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      for: 15m
      labels:
        page: "true"
        severity: critical
  - name: kubernetes-system
    rules:
    - alert: KubeVersionMismatch
      annotations:
        message: There are {{ $value }} different semantic versions of Kubernetes
          components running.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
      expr: |
        count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-system-apiserver
    rules: []
  - name: kubernetes-system-kubelet
    rules:
    - alert: KubeNodeNotReady
      annotations:
        message: '{{ $labels.node }} has been unready for more than 15 minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
      expr: |
        kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeNodeUnreachable
      annotations:
        message: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable
      expr: |
        kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
      for: 2m
      labels:
        severity: warning
    - alert: KubeletTooManyPods
      annotations:
        message: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
          }} of its Pod capacity.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
      expr: |
        max(max(kubelet_running_pod_count{job="kubelet"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job="kubelet"}) by(node) / max(kube_node_status_capacity_pods{job="kube-state-metrics"}) by(node) > 0.95
      for: 15m
      labels:
        severity: warning
    - alert: KubeletDown
      annotations:
        message: Kubelet has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
      expr: |
        absent(up{job="kubelet"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
  - name: kubernetes-system-scheduler
    rules: []
  - name: kubernetes-system-controller-manager
    rules: []
  - name: prometheus
    rules:
    - alert: PrometheusBadConfig
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          reload its configuration.
        summary: Failed Prometheus configuration reload.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="monitoring"}[5m]) == 0
      for: 10m
      labels:
        page: "false"
        severity: critical
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
          is running full.
        summary: Prometheus alert notification queue predicted to run full in less
          than 30m.
      expr: |
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="monitoring"}[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
      annotations:
        description: '{{ printf "%.1f" $value }}% errors while sending alerts from
          Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to
          a specific Alertmanager.
      expr: |
        (
          rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        /
          rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
      annotations:
        description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts
          from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
        summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      expr: |
        min without(alertmanager) (
          rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        /
          rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
        * 100
        > 3
      for: 15m
      labels:
        page: "false"
        severity: critical
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
          to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="monitoring"}[5m]) < 1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
          {{$value | humanize}} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      expr: |
        increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
          {{$value | humanize}} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      expr: |
        increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
          samples.
        summary: Prometheus is not ingesting samples.
      expr: |
        rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="monitoring"}[5m]) <= 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusDuplicateTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
          {{ printf "%.4g" $value  }} samples/s with different values but duplicated
          timestamp.
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOutOfOrderTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
          {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusRemoteWriteDesiredShards
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
          desired shards calculation wants to run {{ $value }} shards, which is more
          than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-k8s",namespace="monitoring"}`
          $labels.instance | query | first | value }}.
        summary: Prometheus remote write desired shards calculation wants to run more
          than configured max shards.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-k8s",namespace="monitoring"}[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusRuleFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          evaluate {{ printf "%.0f" $value }} rules in the last 5m.
        summary: Prometheus is failing rule evaluations.
      expr: |
        increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        page: "false"
        severity: critical
    - alert: PrometheusMissingRuleEvaluations
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
          printf "%.0f" $value }} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: |
        increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
  - name: blackbox.alerts
    rules:
    - alert: SiteIsDown
      annotations:
        description: Site {{$labels.target}} has been down (non 2xx code) for 1 minute
          according to {{ $value }}% of Blackbox probes
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/SiteIsDown.md
        summary: Site {{$labels.target}} is down (non 2xx code)
      expr: 100 * count by (target,job,app_mintel_com_owner) (probe_success{job="blackbox"}
        < 1) / blackbox_node_count >= 50
      for: 1m
      labels:
        severity: warning
    - alert: SiteIsDown
      annotations:
        description: Site {{$labels.target}} has been down (non 2xx code) for more
          than 3 minutes according to {{ $value }}% of Blackbox probes
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/SiteIsDown.md
        summary: Site {{$labels.target}} is down (non 2xx code)
      expr: 100 * count by (target,job,app_mintel_com_owner) (probe_success{job="blackbox"}
        < 1) / blackbox_node_count >= 50
      for: 3m
      labels:
        page: "true"
        severity: critical
    - alert: SiteStatusIsFlapping
      annotations:
        description: Site {{$labels.target}} status (non 2xx code) has changed more
          than 5 times over the last 10 minutes according to {{ $value }}% of Blackbox
          probes
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/SiteStatusIsFlapping.md
        summary: Site {{$labels.target}} status (non 2xx code) is flapping
      expr: 100 * count by (target,job,app_mintel_com_owner) (changes(probe_success{job="blackbox"}[10m])
        > 5) / blackbox_node_count >= 50
      for: 5m
      labels:
        page: "true"
        severity: critical
    - alert: TargetIsDown
      annotations:
        description: Target {{$labels.target}} has been down for 5 minutes according
          to {{ $value }}% of Blackbox probes
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/TargetIsDown.md
        summary: Target {{$labels.target}} is down
      expr: 100 * count by (target,job,app_mintel_com_owner) (up{job="blackbox"} ==
        0) / blackbox_node_count >= 50
      for: 5m
      labels:
        page: "true"
        severity: critical
    - alert: TargetSSLCertExpireNear
      annotations:
        description: Target {{$labels.target}} SSL Certificate is due to expire in
          less than 29 days
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/TargetSSLCertExpireNear.md
        summary: Target {{$labels.target}} SSL Certificate is due to expire in less
          than 29 days
      expr: (min by(target,job,app_mintel_com_owner) (probe_ssl_earliest_cert_expiry{}
        - time()) ) / 60 / 60 / 24 < 29
      for: 24h
      labels:
        severity: warning
    - alert: TargetSSLCertExpireNear
      annotations:
        description: Target {{$labels.target}} SSL Certificate is due to expire in
          less than 14 days
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/TargetSSLCertExpireNear.md
        summary: Target {{$labels.target}} SSL Certificate is due to expire in less
          than 14 days
      expr: (min by(target,job,app_mintel_com_owner) (probe_ssl_earliest_cert_expiry{}
        - time()) ) / 60 / 60 / 24 < 14
      for: 1h
      labels:
        page: "true"
        severity: critical
  - name: elasticsearch.alerts
    rules:
    - alert: ElasticsearchOperatorReconcileErrors
      annotations:
        description: ElasticSearch Operator failing to reconcile controller {{ $labels.controller
          }}
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchOperatorReconcileErrors.md
        summary: ElasticSearch Operator failing to reconcile controller {{ $labels.controller
          }}
      expr: increase(controller_runtime_reconcile_errors_total{job="elastic-operator-metrics"}[30m])
        >0
      for: 30m
      labels:
        severity: warning
    - alert: ElasticsearchHeapTooHigh
      annotations:
        description: ElasticSearch instance {{$labels.name}} heap usage is high in
          cluster {{$labels.cluster}}
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchHeapTooHigh.md
        summary: The heap usage is over 80% for 15m
      expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}
        > 0.8
      for: 15m
      labels:
        severity: warning
    - alert: ElasticsearchHeapTooHigh
      annotations:
        description: ElasticSearch instance {{$labels.name}} heap usage is high in
          cluster {{$labels.cluster}}
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchHeapTooHigh.md
        summary: The heap usage is over 90% for 15m
      expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}
        > 0.9
      for: 15m
      labels:
        page: "false"
        severity: critical
    - alert: ElasticsearchLowDiskFree
      annotations:
        description: Free Disk for elasticsearch node is lower than 30% on {{$labels.name}}
          in cluster {{$labels.cluster}}
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchLowDiskFree.md
        summary: Low free Disk for elasticsearch node
      expr: elasticsearch_filesystem_data_free_percent < 30
      for: 15m
      labels:
        severity: warning
    - alert: ElasticsearchLowDiskFree
      annotations:
        description: Free Disk for elasticsearch node is lower than 15% on {{$labels.name}}
          in cluster {{$labels.cluster}}
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchLowDiskFree.md
        summary: Low free Disk for elasticsearch node
      expr: elasticsearch_filesystem_data_free_percent < 15
      for: 15m
      labels:
        page: "false"
        severity: critical
    - alert: ElasticsearchClusterHealthRED
      annotations:
        description: 'Cluster is RED: not all primary and replica shards are allocated
          in elasticsearch cluster {{ $labels.cluster }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchClusterHealthRED.md
        summary: Cluster {{ $labels.cluster }} health is RED
      expr: elasticsearch_cluster_health_status{color="red"}==1
      for: 5m
      labels:
        page: "true"
        severity: critical
    - alert: ElasticsearchClusterHealthYELLOW
      annotations:
        description: 'Cluster is YELLOW: not all replica shards are allocated in elasticsearch
          cluster {{ $labels.cluster }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchClusterHealthYELLOW.md
        summary: Cluster {{ $labels.cluster }} health is YELLOW
      expr: elasticsearch_cluster_health_status{color="yellow"}==1
      for: 120m
      labels:
        page: "false"
        severity: critical
    - alert: ElasticsearchClusterHealthUp
      annotations:
        description: ElasticSearch {{ $labels.cluster }} last scrape of the ElasticSearch
          cluster health failed
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchClusterHealthUp.md
        summary: ElasticSearch {{ $labels.cluster }} last scrape of the ElasticSearch
          cluster health failed
      expr: elasticsearch_cluster_health_up !=1
      for: 3m
      labels:
        severity: warning
    - alert: ElasticsearchClusterHealthUp
      annotations:
        description: ElasticSearch {{ $labels.cluster }} last scrape of the ElasticSearch
          cluster health failed
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchClusterHealthUp.md
        summary: ElasticSearch {{ $labels.cluster }} last scrape of the ElasticSearch
          cluster health failed
      expr: elasticsearch_cluster_health_up !=1
      for: 5m
      labels:
        page: "true"
        severity: critical
    - alert: ElasticsearchGCRunsCount
      annotations:
        description: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: Count of JVM GC runs > 5 per sec and has a value of {{ $value }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchGCRunsCount.md
        summary: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: Count of JVM GC runs > 5 per sec and has a value of {{ $value }}'
      expr: rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m])>7
      for: 3m
      labels:
        severity: warning
    - alert: ElasticsearchGCRunTime
      annotations:
        description: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: GC run time in seconds > 0.3 sec and has a value of {{ $value }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ElasticsearchGCRunTime.md
        summary: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: GC run time in seconds > 0.3 sec and has a value of {{ $value }}'
      expr: rate(elasticsearch_jvm_gc_collection_seconds_sum[5m])>0.3
      for: 3m
      labels:
        severity: warning
    - alert: Elasticsearch_breakers_tripped
      annotations:
        description: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: breakers tripped > 0 and has a value of {{ $value }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/Elasticsearch_breakers_tripped.md
        summary: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: breakers tripped > 0 and has a value of {{ $value }}'
      expr: rate(elasticsearch_breakers_tripped{}[5m])>0
      for: 3m
      labels:
        severity: warning
    - alert: Elasticsearch_health_timed_out
      annotations:
        description: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: Number of cluster health checks timed out > 0 and has a value of {{
          $value }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/Elasticsearch_health_timed_out.md
        summary: 'ElasticSearch node {{ $labels.name }} on cluster {{ $labels.cluster
          }}: Number of cluster health checks timed out > 0 and has a value of {{
          $value }}'
      expr: elasticsearch_cluster_health_timed_out>0
      for: 3m
      labels:
        severity: warning
    - alert: Elasticsearch_json_parse_failures
      annotations:
        description: 'ElasticSearch node {{ $labels.instance }}: json parse failures
          over an hour > 3 and has a value of {{ $value }}'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/Elasticsearch_json_parse_failures.md
        summary: 'ElasticSearch node {{ $labels.instance }}: json parse failures over
          an hour > 3 and has a value of {{ $value }}'
      expr: increase(elasticsearch_cluster_health_json_parse_failures[1h]) > 3
      for: 5m
      labels:
        severity: warning
  - name: haproxy-ingress.alerts
    rules:
    - alert: HAProxyFrontendSessionUsage
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyFrontendSessionUsage
        summary: 'HAProxy: Session usage on {{ $labels.frontend }} frontend has reached
          {{ $value }}%'
      expr: haproxy:http_frontend_session_usage:percentage >= 80
      for: 15m
      labels:
        severity: warning
    - alert: HAProxyFrontendSessionUsage
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyFrontendSessionUsage
        summary: 'HAProxy: Session usage on {{ $labels.frontend }} frontend has reached
          {{ $value }}%'
      expr: haproxy:http_frontend_session_usage:percentage >= 90
      for: 15m
      labels:
        page: "false"
        severity: critical
    - alert: HAProxyFrontendSessionUsageWillRunOut
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyFrontendSessionUsage
        summary: 'HAProxy: Session usage on {{ $labels.frontend }} frontend has reached
          {{ $value }}% and will run out in the next 2h'
      expr: "haproxy:http_frontend_session_usage:percentage >= 90 \nand\npredict_linear(haproxy:http_frontend_session_usage:percentage[1h],
        2*60*60) >= 100\n"
      for: 10m
      labels:
        page: "true"
        severity: critical
    - alert: HAProxyFrontendRequestErrors
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyFrontendRequestErrors
        summary: 'HAProxy: Request error rate increase detected on {{ $labels.frontend
          }} frontend'
      expr: sum by (frontend) (rate(haproxy_frontend_request_errors_total{frontend!~"stats|healthz|error503noendpoints|.*default-backend"}[2m]))
        / sum by (frontend) (rate(haproxy_frontend_http_requests_total[2m])) * 100
        > 1
      for: 5m
      labels:
        severity: warning
    - alert: HAProxyFrontendRequestErrors
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyFrontendRequestErrors
        summary: 'HAProxy: Request error rate increase detected on {{ $labels.frontend
          }} frontend'
      expr: sum by (frontend) (rate(haproxy_frontend_request_errors_total{frontend!~"stats|healthz|error503noendpoints|.*default-backend"}[2m]))
        / sum by (frontend) (rate(haproxy_frontend_http_requests_total[2m])) * 100
        > 5
      for: 10m
      labels:
        page: "true"
        severity: critical
    - alert: HAProxyBackendRequestQueued
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendRequestQueued
        summary: 'HAProxy: Request are queuing up on the {{ $labels.mintel_com_service
          }} backend'
      expr: sum by (mintel_com_service, label_app_mintel_com_owner) (haproxy:haproxy_backend_current_queue:labeled)
        > 10
      for: 2m
      labels:
        severity: warning
    - alert: HAProxyBackendRequestQueued
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendRequestQueued
        summary: 'HAProxy: Request are queuing up on the {{ $labels.mintel_com_service
          }} backend'
      expr: sum by (mintel_com_service, label_app_mintel_com_owner) (haproxy:haproxy_backend_current_queue:labeled)
        > 100
      for: 10m
      labels:
        page: "true"
        severity: critical
    - alert: HAProxyBackendRequestQueuedTime
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendRequestQueuedTime
        summary: 'HAProxy: Excessive request queue time on the {{ $labels.mintel_com_service
          }} backend'
      expr: haproxy:http_backend_queue_time_seconds_bucket:histogram_quantile > 0.3
      for: 2m
      labels:
        severity: warning
    - alert: HAProxyBackendRequestQueuedTime
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendRequestQueuedTime
        summary: 'HAProxy: Excessive request queue time on the {{ $labels.mintel_com_service
          }} backend'
      expr: haproxy:http_backend_queue_time_seconds_bucket:histogram_quantile > 0.5
      for: 10m
      labels:
        page: "true"
        severity: critical
    - alert: HAProxyBackendResponseErrors
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendResponseErrors
        summary: 'HAProxy: Response errors detected on {{ $labels.mintel_com_service
          }} backend'
      expr: sum by (mintel_com_service, label_app_mintel_com_owner) (rate(haproxy:haproxy_backend_response_errors_total:labeled[1m]))
        > 1
      for: 2m
      labels:
        severity: warning
    - alert: HAProxyBackendResponseErrors
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendResponseErrors
        summary: 'HAProxy: Response errors detected on {{ $labels.mintel_com_service
          }} backend'
      expr: sum by (mintel_com_service, label_app_mintel_com_owner) (rate(haproxy:haproxy_backend_response_errors_total:labeled[1m]))
        > 10
      for: 10m
      labels:
        page: "false"
        severity: critical
    - alert: HAProxyBackendResponseErrors5xx
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendResponseErrors5xx
        summary: 'HAProxy: Increased number of 5xx responses on {{ $labels.mintel_com_service
          }} service'
      expr: |
        (
        sum by (mintel_com_service, label_app_mintel_com_owner) (rate(haproxy:haproxy_backend_http_responses_total:labeled{code=~"5.."}[1m]))
        /
        sum by (mintel_com_service, label_app_mintel_com_owner) (rate(haproxy:haproxy_backend_http_responses_total:labeled[1m]))
        ) * 100 > 1
      for: 5m
      labels:
        severity: warning
    - alert: HAProxyBackendResponseErrors5xx
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendResponseErrors5xx
        summary: 'HAProxy: Increased number of 5xx responses on {{ $labels.mintel_com_service
          }} service'
      expr: |
        (
        sum by (mintel_com_service, label_app_mintel_com_owner) (rate(haproxy:haproxy_backend_http_responses_total:labeled{code=~"5.."}[1m]))
        /
        sum by (mintel_com_service, label_app_mintel_com_owner) (rate(haproxy:haproxy_backend_http_responses_total:labeled[1m]))
        ) * 100 > 10
      for: 10m
      labels:
        page: "true"
        severity: critical
    - alert: HAProxyBackendDown
      annotations:
        description: 'HAProxy: {{ $labels.mintel_com_service }} backend has been down
          for at least 1m on all Ingress pods'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyBackendDown
      expr: sum by (mintel_com_service, label_app_mintel_com_owner) (haproxy:haproxy_backend_up:labeled)
        == 0
      for: 5m
      labels:
        page: "false"
        severity: critical
    - alert: HAProxyServerInBackendUpPercentageLow
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyServerInBackendUpPercentageLow
        summary: 'HAProxy: The percentage of server up in backend for {{ $labels.mintel_com_service
          }} service is low on ingress {{ $labels.pod }} : {{ $value }}%'
      expr: "(\nsum by (mintel_com_service, label_app_mintel_com_owner, pod) (haproxy:haproxy_server_up:labeled)\n/\ncount
        by (mintel_com_service, label_app_mintel_com_owner, pod) (haproxy:haproxy_server_up:labeled)\n)
        * 100 < 75 \n"
      for: 5m
      labels:
        severity: warning
    - alert: HAProxyServerInBackendUpPercentageLow
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyServerInBackendUpPercentageLow
        summary: 'HAProxy: The percentage of server up in backend for {{ $labels.mintel_com_service
          }} service is low on ingress {{ $labels.pod }} : {{ $value }}%'
      expr: |
        (
        sum by (mintel_com_service, label_app_mintel_com_owner, pod) (haproxy:haproxy_server_up:labeled)
        /
        count by (mintel_com_service, label_app_mintel_com_owner, pod) (haproxy:haproxy_server_up:labeled)
        ) * 100 < 50
      for: 5m
      labels:
        page: "false"
        severity: critical
    - alert: HAProxyFrontendIncreasedRequests
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#HAProxyFrontendIncreasedRequests
        summary: 'HAProxy: The number of requests on frontend {{ $labels.frontend
          }} has increades by {{ $value }} in the last 10m minutes compared to the
          0.9 quantile over the last 24h'
      expr: "100 * \navg by (frontend) ( \n  rate(haproxy_frontend_http_requests_total{frontend!~\"stats|healthz|.*default-backend\"}[10m])
        /\n  quantile_over_time(0.9, rate(haproxy_frontend_http_requests_total{frontend!~\"stats|healthz|.*default-backend\"}[10m])[24h:10m])\n)
        > 500\n"
      for: 5m
      labels:
        severity: warning
  - name: image-service.alerts
    rules:
    - alert: ImageServiceHAProxyBackendResponseTime
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#haproxybackendresponsetime
        summary: 'HAProxy: Average response times increased on {{ $labels.mintel_com_service
          }} backend.'
      expr: |
        haproxy:http_backend_response_wait_seconds_bucket:histogram_quantile{label_app_mintel_com_owner="moat", mintel_com_service=~".*image-service.*"} > 5
      for: 2m
      labels:
        page: "true"
        severity: critical
    - alert: ImageServiceHAProxyBackendResponseTime
      annotations:
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/HAProxy.md#haproxybackendresponsetime
        summary: 'HAProxy: Average response times increased on {{ $labels.mintel_com_service
          }} backend.'
      expr: |
        haproxy:http_backend_response_wait_seconds_bucket:histogram_quantile{label_app_mintel_com_owner="moat", mintel_com_service=~".*image-service.*"} > 1
      for: 5m
      labels:
        severity: warning
    - alert: ImageServiceAuthIssues
      annotations:
        message: Service {{ $labels.service }} is returning auth issues fo {{ $value
          }}% of requests (2% threshold)
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/image-service/ImageServiceAuthIssues.md
      expr: |
        sum(rate(thumbor_response_status_total{service=~".*image-service.*", statuscode=~"^(403)$"}[5m])) by (service, app_mintel_com_owner)
        /
        sum(rate(thumbor_response_status_total{service=~".*image-service.*"}[5m])) by (service, app_mintel_com_owner)
        * 100 > 2
      for: 5m
      labels:
        severity: warning
    - alert: ImageServiceAuthIssues
      annotations:
        message: Service {{ $labels.service }} is returning auth issues for {{ $value
          }}% of requests (5% threshold)
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/image-service/ImageServiceAuthIssues.md
      expr: |
        sum(rate(thumbor_response_status_total{service=~".*image-service.*", statuscode=~"^(403)$"}[5m])) by (service, app_mintel_com_owner)
        /
        sum(rate(thumbor_response_status_total{service=~".*image-service.*"}[5m])) by (service, app_mintel_com_owner)
        * 100 > 5
      for: 2m
      labels:
        page: "true"
        severity: critical
  - name: git-sync.alerts
    rules:
    - alert: GitSyncFailing
      annotations:
        description: Git-sync sidecar in the pod {{ $labels.pod }} has failed 45 times
          during the last hour.
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/GitSyncFailing.md
        summary: Git-sync failing to pull git repos
      expr: increase(git_sync_count_total{status="error"}[60m]) > 45
      for: 5m
      labels:
        severity: warning
  - name: kubernetes-resources-mintel.alerts
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        message: Cluster has overcommitted CPU resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.cpu"})
          /
        sum(node:node_num_cpu:sum)
          > 3
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemOvercommit
      annotations:
        message: Cluster has overcommitted memory resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.memory"})
          /
        sum(node_memory_MemTotal_bytes{job="node-exporter"})
          > 3
      for: 5m
      labels:
        severity: warning
    - alert: KubePodDistributionUnbalancedByNode
      annotations:
        description: 'Pod Distribution for: {{ $labels.created_by_kind }}/{{ $labels.created_by_name}}
          , {{ $value }}% are on node {{ $labels.node }}'
        message: Pod Distribution for pods is unbalanced by node
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/KubePodDistributionUnbalancedByNode.md
      expr: "100 * (\n  (count by (created_by_kind, created_by_name, node ) (kube_pod_info{created_by_kind!~\"<none>|Job\"})
        > 1) \n  / \n  ignoring(node) \n    group_left(created_by_kind, created_by_name)
        \n      count by (created_by_kind, created_by_name) (kube_pod_info{created_by_kind!~\"<none>|Job\"})\n)
        > 50\n"
      for: 15m
      labels:
        severity: warning
  - name: mintel-containers.alerts
    rules:
    - alert: ContainerCombinedIoHighOverTime
      annotations:
        description: Container {{ $labels.container_name }} in Pod {{ $labels.pod_name
          }} have been doing an unusual amount of Sustained IO on {{ $labels.device
          }} for the specified time
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ContainerCombinedIoHighOverTime.md
        summary: Container have been doing an unusual amount of IO
      expr: |
        rate(container_fs_reads_bytes_total{job="kubelet",device="/dev/sda"}[5m])
        +
        rate(container_fs_writes_bytes_total{job="kubelet",device="/dev/sda"}[5m])
        > 20000000
      for: 12h
      labels:
        page: "false"
        severity: critical
  - name: mintel-disks.alerts
    rules:
    - alert: KubePersistentVolumeInodeUsageCritical
      annotations:
        description: The free space for device {{ $labels.device }} on node {{ $labels.instance
          }} is Predicted to be less than 5% in the next 3 hours at the current rate
          based on the last 4h samples
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/KubePersistentVolumeInodeUsageCritical.md
        summary: The persistent volume {{ $labels.persistentvolumeclaim }} in namespsace
          {{ $labels.exported_namespace }} has {{ $value }}% inodes left
      expr: |
        mintel:pvc:inodes_free:percentage <= 3
      for: 1h
      labels:
        page: "false"
        severity: critical
    - alert: KubePersistentVolumeInodePredictedUsageCritical
      annotations:
        description: |-
          The persistent volume {{ $labels.persistentvolumeclaim }} in namespsace
                          {{ $labels.exported_namespace }} is predicted to use all its inodes within the
                          next 4 days
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/KubePersistentVolumeInodePredictedUsageCritical.md
        summary: Persistent Volume inodes predicted to fill up
      expr: |
        mintel:pvc:inodes_free:percentage < 15 and
        predict_linear(mintel:pvc:inodes_free:percentage[4h], 4 * 24 * 3600) <= 0
      for: 1h
      labels:
        severity: warning
    - alert: KubePersistentVolumeInodePredictedUsageCritical
      annotations:
        description: |-
          The persistent volume {{ $labels.persistentvolumeclaim }} in namespsace
                          {{ $labels.exported_namespace }} is predicted to use all its inodes within the
                          next 4 hours
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/KubePersistentVolumeInodePredictedUsageCritical.md
        summary: Persistent Volume inodes predicted to fill up
      expr: |
        mintel:pvc:inodes_free:percentage < 15 and
        predict_linear(mintel:pvc:inodes_free:percentage[4h], 4 * 60 * 60) <= 0
      for: 10m
      labels:
        page: "true"
        severity: critical
    - alert: KubePersistentVolumeFullInFourHours
      annotations:
        message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is expected to fill up within four
          hours. Currently {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * (
          kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
        ) < 20
        and
        predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 60 * 60) < 0
      for: 15m
      labels:
        page: "true"
        severity: critical
  - name: mintel-node.alerts
    rules:
    - alert: NodeFreeConntrackEntriesLow
      annotations:
        description: 'Free Conntrack entries is less than 10% on this node: {{ $labels.instance
          }} : {{ $value }}%'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/NodeFreeConntrackEntriesLow.md
        summary: Free Conntrack entries is less than 10% on this node
      expr: node:conntrack_entries_free:percentage < 10
      for: 30m
      labels:
        context: node
        severity: warning
    - alert: NodeFreeConntrackEntriesLow
      annotations:
        description: 'Free Conntrack entries is less than 5% on this node: {{ $labels.instance
          }} : {{ $value }}%'
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/NodeFreeConntrackEntriesLow.md
        summary: Free Conntrack entries is less than 5% on this node
      expr: node:conntrack_entries_free:percentage < 5
      for: 30m
      labels:
        context: node
        page: "true"
        severity: critical
    - alert: ClockSkewDetected
      annotations:
        message: Clock skew detected on node-exporter {{ $labels.namespace }}/{{ $labels.pod
          }}. Ensure NTP is configured correctly on this host.
      expr: |
        abs(node_timex_offset_seconds{job="node-exporter"}) > 0.05
      for: 10m
      labels:
        severity: warning
  - name: mintel-overcommit.alerts
    rules:
    - alert: ClusterLowMemoryAvailable
      annotations:
        description: There is only {{ $value }}% allocatable memory remaining across
          all cluster nodes
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/ClusterLowMemoryAvailable.md
        summary: Low memory available across all cluster nodes
      expr: 100 * sum(node:memory_for_pod_available:bytes{node!~"master.*"}) / sum(node:node_memory_bytes_total:sum{node!~"master.*"})
        < 10
      for: 30m
      labels:
        context: cluster
        page: "false"
        severity: critical
  - name: mintel-pod.alerts
    rules:
    - alert: PodOOMKilled
      annotations:
        description: Container {{ $labels.container }} in pod {{ $labels.pod }} has
          been OOMKilled AT LEAST {{ $value }} times in the last hour
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/PodOOMKilled.md
        summary: Pod is being OOMKilled
      expr: sum_over_time(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[1h])
        > 3
      for: 5m
      labels:
        page: "false"
        severity: critical
    - alert: KubePodFailed
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a Failed
          state for longer than an hour.
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/KubePodFailed.md
      expr: |
        sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed"}) > 0
      for: 1h
      labels:
        page: "true"
        severity: critical
    - alert: KubePodImageMismatch
      annotations:
        message: Workload {{ $labels.namespace }}/{{ $labels.created_by_kind }}/{{
          $labels.created_by_name }} has {{ $value }} different image versions running
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/KubePodImageMismatch.md
      expr: |
        count(
          count(
            (kube_pod_container_info) * on(pod)
            group_left(created_by_kind, created_by_name) (kube_pod_info{created_by_kind!~"<none>|Job"})
          )
          by (created_by_kind, created_by_name, image_id, container, namespace)
        )
        by (created_by_kind, created_by_name, container, namespace)
        > 1
      for: 30m
      labels:
        severity: warning
  - name: mintel-web-frontend.alerts
    rules:
    - alert: MintelWebServiceDown
      annotations:
        description: Service `{{$labels.service}}`is down.
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/MintelWebServiceDown.md
      expr: count(mintel:web_frontend:check_up == 1) by (job, namespace, service,
        environment, label_app_mintel_com_owner) == 0
      for: 3m
      labels:
        page: "true"
        severity: critical
    - alert: MintelWebServiceDown
      annotations:
        description: Service `{{$labels.service}}`is down.
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/MintelWebServiceDown.md
      expr: count(mintel:web_frontend:check_up == 1) by (job, namespace, service,
        environment, label_app_mintel_com_owner) == 0
      for: 1m
      labels:
        severity: warning
    - alert: MintelReducedService
      annotations:
        description: Service `{{$labels.service}}`is down.
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/MintelReducedService.md
      expr: count(mintel:web_frontend:check_up == 1) by (job, namespace, service,
        environment, label_app_mintel_com_owner) == 1
      for: 3m
      labels:
        page: "true"
        severity: critical
    - alert: MintelReducedService
      annotations:
        description: Service `{{$labels.service}}`is down.
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/MintelReducedService.md
      expr: count(mintel:web_frontend:check_up == 1) by (job, namespace, service,
        environment, label_app_mintel_com_owner) == 1
      for: 1m
      labels:
        severity: warning
  - name: flux.alerts
    rules:
    - alert: FluxDaemonGeneralSyncError
      annotations:
        description: General flux sync errors
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/flux.md#fluxdaemongeneralsyncerror
        summary: General Flux Daemon sync error
      expr: delta(flux_daemon_sync_duration_seconds_count{job="flux",success="true"}[6m])
        < 1
      for: 120m
      labels:
        severity: warning
    - alert: FluxManifestsSyncError
      annotations:
        description: Flux manifests sync errors
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/flux.md#fluxmanifestssyncerror
        summary: Flux Manifests sync error
      expr: flux_daemon_sync_manifests{job="flux",success="false"} > 0
      for: 60m
      labels:
        severity: warning
    - alert: FluxMemCacheErrors
      annotations:
        description: Flux Errors talking to memcached
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/flux.md#fluxmemcacheerrors
        summary: Flux Errors talking to memcached
      expr: delta(flux_cache_request_duration_seconds_count{job="flux",success="false"}[12m])
        > 0
      for: 60m
      labels:
        severity: warning
    - alert: FluxRemoteFetchErrors
      annotations:
        description: Flux errors fetching from remote registry
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/flux.md#fluxremotefetcherrors
        summary: Flux errors fetching from remote registry
      expr: delta(flux_client_fetch_duration_seconds_count{job="flux",success="false"}[12m])
        > 0
      for: 240m
      labels:
        severity: warning
    - alert: FluxReleaseErrors
      annotations:
        description: Flux image release errors
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/flux.md#fluxreleaseerrors
        summary: Flux release errors
      expr: delta(flux_fluxsvc_release_duration_seconds_count{job="flux",success="false"}[6m])
        > 0
      for: 10m
      labels:
        severity: warning
  - name: fluentd.alerts
    rules:
    - alert: FluentdHighOutputRetry
      annotations:
        description: Fluentd number of output retry has been increasing for 1h
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/FluentdHighOutputRetry.md
        summary: Fluentd number of output retry has been increasing for 1h
      expr: sum by (service,pod,type) (increase(fluentd_output_status_retry_count{type!~"^(null|rewrite_tag_filter|detect_exceptions)$"}[5m]))
        >0
      for: 1h
      labels:
        severity: warning
  - name: prometheus-operator.alerts
    rules:
    - alert: PrometheusOperatorRulesValidationErrors
      annotations:
        description: Prometheus Operator failed to validate rules
        runbook_url: https://gitlab.com/mintel/satoshi/docs/blob/master/runbooks/core/PrometheusOperatorRulesValidationErrors.md
        summary: Prometheus Operator failed to validate rules
      expr: increase(prometheus_operator_rule_validation_errors_total{job="prometheus-operator"}[5m])
        >0
      for: 30m
      labels:
        severity: warning
  - name: kubernetes-absent
    rules:
    - alert: AlertmanagerDown
      annotations:
        message: Alertmanager has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="alertmanager-main",namespace="monitoring"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: CoreDNSDown
      annotations:
        message: CoreDNS has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="kube-dns"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: HaproxyIngressDown
      annotations:
        message: HaproxyIngress has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="haproxy-exporter"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeStateMetricsDown
      annotations:
        message: KubeStateMetrics has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="kube-state-metrics"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: KubeletDown
      annotations:
        message: Kubelet has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="kubelet"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: NodeExporterDown
      annotations:
        message: NodeExporter has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="node-exporter"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: PrometheusDown
      annotations:
        message: Prometheus has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="prometheus-k8s",namespace="monitoring"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
    - alert: PrometheusOperatorDown
      annotations:
        message: PrometheusOperator has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="prometheus-operator",namespace="monitoring"} == 1)
      for: 15m
      labels:
        page: "true"
        severity: critical
